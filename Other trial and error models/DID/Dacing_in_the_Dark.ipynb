{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9A4TrRIoK5MH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import VGG\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PreF667fJV0u"
      },
      "outputs": [],
      "source": [
        "depth, channels, height, width = 8, 16, 64, 64  # Example input dimensions\n",
        "input_video = torch.randn(depth, channels, height, width)  # Random low-light video tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "_NLUtrwaJl-e",
        "outputId": "2f0d0eb1-2e34-4e08-c453-2a603c5d0062"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, nf=8):  # nf is the channel multiplier\n",
        "        super(Encoder, self).__init__()\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv3d(3, nf, kernel_size=3, stride=2, padding=1),  # Conv Layer 0\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.InstanceNorm3d(nf),\n",
        "\n",
        "            nn.Conv3d(nf, nf * 2, kernel_size=3, stride=2, padding=1),  # Conv Layer 1\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.InstanceNorm3d(nf * 2),\n",
        "\n",
        "            nn.Conv3d(nf * 2, nf * 4, kernel_size=3, stride=2, padding=1),  # Conv Layer 2\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.InstanceNorm3d(nf * 4),\n",
        "\n",
        "            nn.Conv3d(nf * 4, nf * 8, kernel_size=3, stride=2, padding=1),  # Conv Layer 3\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.InstanceNorm3d(nf * 8),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoAPknMP9qLC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SGvMf1eZKajN"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define U-Net model for the illumination refinement\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG, self).__init__()\n",
        "        self.model = VGG()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Custom illumination enhancement module with iterative refinement\n",
        "class IlluminationEnhancementModule(nn.Module):\n",
        "    def __init__(self, T, encoder):\n",
        "        super(IlluminationEnhancementModule, self).__init__()\n",
        "        self.T = T  # Number of iterations\n",
        "        self.VGG = VGG()  # U-Net for the iterative refinement\n",
        "        self.encoder = Encoder()  # Pre-trained encoder for feature extraction\n",
        "\n",
        "    def forward(self, Xt, Yt):\n",
        "        # Latent representation z0 (initial illumination representation)\n",
        "        z0 = self.encoder(Xt)\n",
        "        zt = z0  # Initialize with z0\n",
        "\n",
        "        # Iterate T times to refine the illumination\n",
        "        for k in range(1, self.T + 1):\n",
        "            alpha_k = k / self.T\n",
        "            # Mix low-light and normal-light frames based on alpha_k\n",
        "            zk = alpha_k * Yt + (1 - alpha_k) * Xt\n",
        "            zt = self.VGG(zk)  # Refining the illumination at step k\n",
        "\n",
        "        # Final refined illumination component zT\n",
        "        return zt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GojugTq0LzWn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 64, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "# refinement layer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the residual block\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out += identity  # Skip connection\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "# Define the function to concatenate and project\n",
        "class Projector(nn.Module):\n",
        "    def __init__(self, num_frames, input_channels=3, output_channels=64):\n",
        "        super(Projector, self).__init__()\n",
        "        self.num_frames = num_frames\n",
        "        self.concat_conv = nn.Conv2d(input_channels * num_frames, output_channels, kernel_size=1)\n",
        "        self.res_block = ResidualBlock(output_channels, output_channels)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # X is of shape (batch_size, num_frames, height, width, channels)\n",
        "        # Concatenate frames along the channel dimension\n",
        "        B, k, H, W, C = X.shape\n",
        "        X_concat = X.view(B, k * C, H, W)  # Resulting shape: (batch_size, 3k, H, W)\n",
        "\n",
        "        # Project through 1x1 convolution to get embedding F0_t\n",
        "        F0_t = self.concat_conv(X_concat)  # Shape: (batch_size, output_channels, H, W)\n",
        "\n",
        "        # Pass through residual block\n",
        "        F0_t = self.res_block(F0_t)  # Shape remains the same\n",
        "\n",
        "        return F0_t\n",
        "\n",
        "# Example usage\n",
        "batch_size = 4\n",
        "num_frames = 5\n",
        "height, width = 64, 64\n",
        "input_channels = 3\n",
        "output_channels = 64\n",
        "\n",
        "# Example input: (batch_size, num_frames, height, width, input_channels)\n",
        "Xt = torch.randn(batch_size, num_frames, height, width, input_channels)\n",
        "\n",
        "# Initialize the projector and run the forward pass\n",
        "projector = Projector(num_frames=num_frames, input_channels=input_channels, output_channels=output_channels)\n",
        "F0_t = projector(Xt)\n",
        "\n",
        "print(F0_t.shape)  # Expected output: (batch_size, output_channels, height, width)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VajnQq0-2zqe"
      },
      "outputs": [],
      "source": [
        "class FeatureExtractionBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(FeatureExtractionBlock, self).__init__()\n",
        "\n",
        "        # First path: LayerNorm -> Depthwise Conv -> Channel Attention\n",
        "        self.layer_norm1 = nn.LayerNorm([in_channels, 1, 1])\n",
        "        self.depthwise_conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, groups=in_channels)\n",
        "        self.channel_attention = nn.Conv2d(in_channels, out_channels, kernel_size=1)  # Channel Attention\n",
        "\n",
        "        # Second path: LayerNorm -> Standard Convolution\n",
        "        self.layer_norm2 = nn.LayerNorm([in_channels, 1, 1])\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First path\n",
        "        x1 = self.layer_norm1(x)\n",
        "        x1 = self.depthwise_conv(x1)\n",
        "        x1 = self.channel_attention(x1)\n",
        "        # out = x1 +\n",
        "        # Second path\n",
        "        x2 = self.layer_norm2(x)\n",
        "        x2 = self.conv(x2)\n",
        "\n",
        "        # Combine both paths (skip connection)\n",
        "        out = x1 + x2\n",
        "\n",
        "        return out\n",
        "\n",
        "class DownsampleBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DownsampleBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "# Reflectance Estimation Module\n",
        "class ReflectanceEstimationModule(nn.Module):\n",
        "    def __init__(self, in_channels, num_stages=4):\n",
        "        super(ReflectanceEstimationModule, self).__init__()\n",
        "\n",
        "        self.num_stages = num_stages\n",
        "        self.feature_blocks = nn.ModuleList()\n",
        "        self.downsample_blocks = nn.ModuleList()\n",
        "\n",
        "        # Define stages\n",
        "        for i in range(num_stages):\n",
        "            # Double channels after each stage (as shown in the diagram)\n",
        "            in_c = in_channels * (2 ** i)\n",
        "            out_c = in_channels * (2 ** (i + 1))\n",
        "\n",
        "            # Feature extraction block and downsample block for each stage\n",
        "            self.feature_blocks.append(FeatureExtractionBlock(in_c, out_c))\n",
        "            self.downsample_blocks.append(DownsampleBlock(in_c, out_c))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i in range(self.num_stages):\n",
        "            # Apply feature extraction\n",
        "            x = self.feature_blocks[i](x)\n",
        "\n",
        "            # Downsample\n",
        "            x = self.downsample_blocks[i](x)\n",
        "\n",
        "        # Final output from the last stage is the reflectance map Rt\n",
        "        return x\n",
        "\n",
        "class ReflectanceIlluminationAlignment(nn.Module):\n",
        "    def __init__(self, reflectance_channels, illumination_channels, out_channels):\n",
        "        super(ReflectanceIlluminationAlignment, self).__init__()\n",
        "\n",
        "        # Alignment convolution layer\n",
        "        self.alignment_conv = nn.Conv2d(reflectance_channels + illumination_channels, out_channels, kernel_size=3, padding=1)\n",
        "    def forward(self, Rt, It):\n",
        "      # Concatenate reflectance and illumination maps along the channel dimension\n",
        "      aligned_input = torch.cat([Rt, It], dim=1)\n",
        "\n",
        "      # Apply alignment convolution to fuse the inputs\n",
        "      aligned_output = self.alignment_conv(aligned_input)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_irFpuw1-EYm"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "      def __init__(self, in_channels, out_channels):\n",
        "        super(Model, self).__init__()\n",
        "        self.IllEnhance = IlluminationEnhancementModule()\n",
        "        self.ResidualBlock = Projector()\n",
        "        self.REM = ReflectanceEstimationModule()\n",
        "        self.Alignment = ReflectanceIlluminationAlignment()\n",
        "\n",
        "      def forward(self, x):\n",
        "        Illcomp = self.IllEnhance(x)\n",
        "        x1 = ResidualBlock(x)\n",
        "        x1 = self.REM(x1)\n",
        "        \n",
        "        \n",
        "        out = self.Alignment(x1, Illcomp)\n",
        "\n",
        "        return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "GPU",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
